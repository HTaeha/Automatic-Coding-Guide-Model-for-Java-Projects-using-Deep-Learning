Using TensorFlow backend.
Standard data
zero :  39451
one :  9019

First data
zero :  39451
one :  9019

Second data
zero :  39451
one :  9019

Third data
zero :  39451
one :  9019

4th data
zero :  39451
one :  9019

hbase-code
After set document size of train data, the number of zero and one label data :  35357 7929
After set document size of test data, the number of zero and one label data :  3973 835

Sentence length Average : 47

Under 10 : 9590
Over 10, Under 30 : 18223
Over 30, Under 100 : 13942
Over 100, Under 150 : 2960
Over 150, Under 200 : 1489
Over 200, Under 400 : 1890
Over 400 : 0

After balance out data.
hbase-code

Sentence length Average : 68

Under 10 : 2515
Over 10, Under 30 : 5018
Over 30, Under 100 : 5851
Over 100, Under 150 : 1797
Over 150, Under 200 : 985
Over 200, Under 400 : 1364
Over 400 : 0

hbase-AST
After set document size of train data, the number of zero and one label data :  35357 7929
After set document size of test data, the number of zero and one label data :  3973 835
After balance out data.
hbase-AST

Sentence length Average : 50

Under 10 : 5670
Over 10, Under 30 : 3789
Over 30, Under 100 : 5147
Over 100, Under 150 : 1395
Over 150, Under 200 : 774
Over 200, Under 400 : 748
Over 400 : 7

hbase-CAST
After set document size of train data, the number of zero and one label data :  35357 7929
After set document size of test data, the number of zero and one label data :  3973 835
After balance out data.
hbase-CAST

Sentence length Average : 120

Under 10 : 2050
Over 10, Under 30 : 4242
Over 30, Under 100 : 4599
Over 100, Under 150 : 1798
Over 150, Under 200 : 1242
Over 200, Under 400 : 2446
Over 400 : 1153

Traceback (most recent call last):
  File "merge_network_input4_new.py", line 250, in <module>
    X_train3, Y_train3 = pr.create_deep_learning_input_data(train_data3, train_logging3, max_sentence_len, embed_size_word2vec, vocabulary, wordvec_model)
  File "/home/2014313303/taeha/JavaAutoLogging/preprocessing_data.py", line 283, in create_deep_learning_input_data
    X = np.empty(shape=[len(data), max_sentence_len, embed_size_word2vec], dtype='float32')
MemoryError
